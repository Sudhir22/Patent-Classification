{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "ALTASharedTask2018.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYfQHE6BFnE",
        "colab_type": "text"
      },
      "source": [
        "#**ALTA Shared Task 2018**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POsrxSQFBFnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emL6vjiQBI53",
        "colab_type": "text"
      },
      "source": [
        "##**Preparing dataframe for the training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQscTTrJBFng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=pd.read_csv('train_data.csv')\n",
        "description=list()\n",
        "for id in train_data.id:\n",
        "    with open('patents/'+str(id)+'.txt',encoding='utf8',errors='ignore') as f:\n",
        "        x=f.read()\n",
        "        description.append(x.strip())\n",
        "train_data['description']=description\n",
        "train_data.first_ipc_mark_section=[ord(x)-64 for x in train_data.first_ipc_mark_section]\n",
        "\n",
        "print(train_data.head())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8jf5Cy2BPHm",
        "colab_type": "text"
      },
      "source": [
        "##**Converting description into tokens (Tokenizing Data)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2IFl_mNBFn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocessor(text):\n",
        "    __tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
        "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
        "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
        "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
        "      | \\.\\.\\.              # ellipsis\n",
        "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
        "    '''\n",
        "\n",
        "    ## call it using tokenizer.tokenize\n",
        "    tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    alphabet_tokens = [token for token in tokens if token.isalpha()]\n",
        "    en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    non_stopwords = [word for word in alphabet_tokens if not word in en_stopwords]\n",
        "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
        "    stems = [str(stemmer.stem(word)) for word in non_stopwords]\n",
        "    \n",
        "    return stems\n",
        "\n",
        "train_data['description_tokens'] = train_data['description'].apply(preprocessor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAX3bh7KBFoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(lowercase = False, \n",
        "                                     tokenizer = lambda x: x, # because we already have tokens available\n",
        "                                     stop_words = None, ## stop words removal already done from NLTK\n",
        "                                     max_features = 150000, ## pick top 100K words by frequency\n",
        "                                     ngram_range = (1, 2), ## we want bigrams now\n",
        "                                     binary = False) ## we do not want as binary/boolean features\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8Xlov3FBX3C",
        "colab_type": "text"
      },
      "source": [
        "##**Create an ML Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ai5BSEbBFoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn import neural_network\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "c=Pipeline(steps=[('bow',bow_vectorizer),\n",
        "                  ('tfidf',TfidfTransformer()),\n",
        "                  ('lr',LogisticRegression(C=40))])\n",
        "\n",
        "\n",
        "msk = np.random.rand(len(train_data)) < 0.75\n",
        "train_X = train_data.description_tokens[msk]\n",
        "test_X = train_data.description_tokens[~msk]\n",
        "y=train_data['first_ipc_mark_section']\n",
        "train_y = y[msk]\n",
        "test_y = y[~msk]\n",
        "\n",
        "c.fit(train_X,train_y)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xJWGYa7BjaH",
        "colab_type": "text"
      },
      "source": [
        "##**Checking the accuracy of trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eLt7PcjBFoy",
        "colab_type": "code",
        "colab": {},
        "outputId": "ae7198be-01bb-4ea2-dd25-9652f0f7d795"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "print(f1_score(pd.Series(test_y).values,c.predict(test_X),average='micro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7018255578093306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0duNq4VlBtut",
        "colab_type": "text"
      },
      "source": [
        "##**Using trained model for prediction/inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-942PBb3BFo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data=pd.read_csv('test_data.csv')\n",
        "description=list()\n",
        "for id in test_data.id:\n",
        "    with open('patents/'+str(id)+'.txt',encoding='utf8',errors='ignore') as f:\n",
        "        x=f.read()\n",
        "        description.append(x.strip())\n",
        "test_data['description']=description\n",
        "#test_data.first_ipc_mark_section=[ord(x)-64 for x in train_data.first_ipc_mark_section]\n",
        "\n",
        "test_data['description_tokens']=test_data['description'].apply(preprocessor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUHXqYoZBFpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data['first_ipc_mark_section']=c.predict(test_data.description_tokens)\n",
        "\n",
        "print(test_data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-PL9W0gBFpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.first_ipc_mark_section=[chr(x+64) for x in test_data.first_ipc_mark_section]\n",
        "print(test_data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qiq70QDBFpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data[['id','first_ipc_mark_section']].to_csv('output.csv',index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}