{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTA Shared Task 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  first_ipc_mark_section  \\\n",
      "0   0                       1   \n",
      "1   1                       7   \n",
      "2   2                       1   \n",
      "3   3                       1   \n",
      "4   4                       4   \n",
      "\n",
      "                                         description  \n",
      "0  ABSTRACT\\n\\n  The disclosure relates to a meth...  \n",
      "1  ABSTRACT\\n\\n       A system and method are pro...  \n",
      "2  ABSTRACT\\n\\nA media module 10 for use with at ...  \n",
      "3  ABSTRACT\\n\\n          A support garment having...  \n",
      "4  THERMALLY REACTIVE THERMOPLASTIC INTERMEDIATE ...  \n"
     ]
    }
   ],
   "source": [
    "#Preparing dataframe for the training data\n",
    "train_data=pd.read_csv('train_data.csv')\n",
    "description=list()\n",
    "for id in train_data.id:\n",
    "    with open('patents/'+str(id)+'.txt',encoding='utf8',errors='ignore') as f:\n",
    "        x=f.read()\n",
    "        description.append(x.strip())\n",
    "train_data['description']=description\n",
    "train_data.first_ipc_mark_section=[ord(x)-64 for x in train_data.first_ipc_mark_section]\n",
    "\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting description into tokens\n",
    "def preprocessor(text):\n",
    "    __tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "    ## call it using tokenizer.tokenize\n",
    "    tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    alphabet_tokens = [token for token in tokens if token.isalpha()]\n",
    "    en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    non_stopwords = [word for word in alphabet_tokens if not word in en_stopwords]\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    stems = [str(stemmer.stem(word)) for word in non_stopwords]\n",
    "    \n",
    "    return stems\n",
    "\n",
    "train_data['description_tokens'] = train_data['description'].apply(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(lowercase = False, \n",
    "                                     tokenizer = lambda x: x, # because we already have tokens available\n",
    "                                     stop_words = None, ## stop words removal already done from NLTK\n",
    "                                     max_features = 150000, ## pick top 100K words by frequency\n",
    "                                     ngram_range = (1, 2), ## we want bigrams now\n",
    "                                     binary = False) ## we do not want as binary/boolean features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=150000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        str...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "c=Pipeline(steps=[('bow',bow_vectorizer),\n",
    "                  ('tfidf',TfidfTransformer()),\n",
    "                  ('lr',LogisticRegression(C=40))])\n",
    "\n",
    "\n",
    "msk = np.random.rand(len(train_data)) < 0.75\n",
    "train_X = train_data.description_tokens[msk]\n",
    "test_X = train_data.description_tokens[~msk]\n",
    "y=train_data['first_ipc_mark_section']\n",
    "train_y = y[msk]\n",
    "test_y = y[~msk]\n",
    "\n",
    "c.fit(train_X,train_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7018255578093306\n"
     ]
    }
   ],
   "source": [
    "#Checking the accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#print(cross_val_score(clf,text_vec_train,y,cv=5))\n",
    "print(f1_score(pd.Series(test_y).values,c.predict(test_X),average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('test_data.csv')\n",
    "description=list()\n",
    "for id in test_data.id:\n",
    "    with open('patents/'+str(id)+'.txt',encoding='utf8',errors='ignore') as f:\n",
    "        x=f.read()\n",
    "        description.append(x.strip())\n",
    "test_data['description']=description\n",
    "#test_data.first_ipc_mark_section=[ord(x)-64 for x in train_data.first_ipc_mark_section]\n",
    "\n",
    "test_data['description_tokens']=test_data['description'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                        description  \\\n",
      "0  3972  MEDIA APPLICATION BACKGROUNDING\\n\\nABSTRACT\\n\\...   \n",
      "1  3973  ABSTRACT\\n\\n      Embodiments of the present d...   \n",
      "2  3974                                  NA\\nparse failure   \n",
      "3  3975  ABSTRACT\\n\\n             Thin, biocompatible, ...   \n",
      "4  3976  5\\n\\n         Abstract\\n\\n         Organic syn...   \n",
      "\n",
      "                                  description_tokens  first_ipc_mark_section  \n",
      "0  [media, applic, background, abstract, method, ...                       8  \n",
      "1  [abstract, embodi, present, disclosur, provid,...                       8  \n",
      "2                                 [na, pars, failur]                       1  \n",
      "3  [abstract, thin, biocompat, composit, materi, ...                       1  \n",
      "4  [abstract, organ, synthesi, raw, materi, valer...                       3  \n"
     ]
    }
   ],
   "source": [
    "test_data['first_ipc_mark_section']=c.predict(test_data.description_tokens)\n",
    "\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                        description  \\\n",
      "0  3972  MEDIA APPLICATION BACKGROUNDING\\n\\nABSTRACT\\n\\...   \n",
      "1  3973  ABSTRACT\\n\\n      Embodiments of the present d...   \n",
      "2  3974                                  NA\\nparse failure   \n",
      "3  3975  ABSTRACT\\n\\n             Thin, biocompatible, ...   \n",
      "4  3976  5\\n\\n         Abstract\\n\\n         Organic syn...   \n",
      "\n",
      "                                  description_tokens first_ipc_mark_section  \n",
      "0  [media, applic, background, abstract, method, ...                      H  \n",
      "1  [abstract, embodi, present, disclosur, provid,...                      H  \n",
      "2                                 [na, pars, failur]                      A  \n",
      "3  [abstract, thin, biocompat, composit, materi, ...                      A  \n",
      "4  [abstract, organ, synthesi, raw, materi, valer...                      C  \n"
     ]
    }
   ],
   "source": [
    "test_data.first_ipc_mark_section=[chr(x+64) for x in test_data.first_ipc_mark_section]\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['id','first_ipc_mark_section']].to_csv('output.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
